{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMP9417: Machine Learning and Data Mining \n",
    "### Group Project (Topic 2)\n",
    "### CommonLit Readability Prize\n",
    "#### Features File\n",
    "Written by WENG XINN CHOW (z5346077) on 24.07.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from readability import getmeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textstat.textstat import textstatistics\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file and print the first 5 rows\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for feature engineering\n",
    "\n",
    "def tokenization(excerpt):\n",
    "    \"\"\"\n",
    "    Tokenize the given excerpt (corpus to list of words) using nltk\n",
    "    \"\"\"\n",
    "    \n",
    "    words = word_tokenize(excerpt)\n",
    "    \n",
    "    return words\n",
    "\n",
    "    \n",
    "def stopwords_count(words):\n",
    "    \"\"\"\n",
    "    Count and return the number of stop words \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a set of English stopwords\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    count = 0\n",
    "    # Count the stopwords \n",
    "    for i in range(len(words)):\n",
    "        if words[i] in stopWords:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "def sentiment_count(words):\n",
    "    \"\"\"\n",
    "    Count and return the number of positive and negative sentiments\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_intensity = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    # Count the positive adjectives \n",
    "    for i in range(len(words)):\n",
    "        # If the compound scroe >= 0.05, it's considered as positive sentiment\n",
    "        if (sentiment_intensity.polarity_scores(words[i])['compound']) >= 0.05:\n",
    "            positive_count += 1\n",
    "        # If the compound scroe <= 0.05, it's considered as negative sentiment\n",
    "        elif (sentiment_intensity.polarity_scores(words[i])['compound']) <= 0.05:\n",
    "            negative_count += 1\n",
    "           \n",
    "    return positive_count, negative_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(excerpt):\n",
    "    \"\"\"\n",
    "    Generate the relevant features related to readability and return all features \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract features using readability library\n",
    "    output = getmeasures(excerpt, lang = 'en')\n",
    "    # Need to tokenize the excerpt before generating features (some features only)\n",
    "    words = tokenization(excerpt)\n",
    "    \n",
    "    # Text info \n",
    "    num_characters = len(excerpt)\n",
    "    num_words = len(excerpt.split(' '))\n",
    "    num_sentences = len(excerpt.split('\\n'))\n",
    "    num_syllables = sum(textstatistics().syllable_count(w) for w in words)\n",
    "    num_unique_words = len(set(excerpt.split(' ')))\n",
    "    unique_to_total = num_unique_words / num_words\n",
    "    # Characters per word and its average\n",
    "    characters_word = [len(w) for w in excerpt.split(' ')]\n",
    "    avg_characters_word = np.mean(characters_word)\n",
    "    # Character per sentence and its average\n",
    "    characters_sentence = [len(s) for s in excerpt.split('\\n')]\n",
    "    avg_characters_sentence = np.mean(characters_sentence)\n",
    "    # Words per sentence and its average\n",
    "    words_sentence = [len(s.split(' ')) for s in excerpt.split('\\n')]\n",
    "    avg_words_sentence = np.mean(words_sentence)\n",
    "    # Syllables per word and its average\n",
    "    syllables_word = [textstatistics().syllable_count(w) for w in words]\n",
    "    avg_syllables_word = np.mean(syllables_word)\n",
    "    # Number of polysyllables (syllables >= 3)\n",
    "    num_polysyllables = [textstatistics().syllable_count(w) >= 3 for w in words].count(True)\n",
    "    num_stopwords = stopwords_count(words)\n",
    "    num_positive_sentiment, num_negative_sentiment = sentiment_count(words)\n",
    "    # Number of long words (len > 6)\n",
    "    num_longwords = [len(w) > 6 for w in excerpt.split(' ')].count(True)\n",
    "    \n",
    "    # Readability Scores (different metrics)\n",
    "    # Flesch Reading Ease\n",
    "    flesch = textstat.flesch_reading_ease(excerpt)\n",
    "    # Flesch Kincaid Grade\n",
    "    kincaid = textstat.flesch_kincaid_grade(excerpt)\n",
    "    # Gunning Fog Index\n",
    "    gunning = textstat.gunning_fog(excerpt)\n",
    "    # SMOG index\n",
    "    smog = textstat.smog_index(excerpt)\n",
    "    # Automated Readability Index \n",
    "    auto = textstat.automated_readability_index(excerpt)\n",
    "    # Coleman-Liau Index\n",
    "    coleman = textstat.coleman_liau_index(excerpt)\n",
    "    # Linsear Write Formula\n",
    "    linsear = textstat.linsear_write_formula(excerpt)\n",
    "    # Dale-Chall Readability Score\n",
    "    dalechall = textstat.dale_chall_readability_score(excerpt)\n",
    "    # LIX\n",
    "    lix = num_longwords / num_words\n",
    "    # RIX\n",
    "    rix = num_longwords / num_sentences\n",
    "    \n",
    "    # Word usage\n",
    "    be_verbs = output['word usage']['tobeverb']\n",
    "    aux_verbs = output['word usage']['auxverb']\n",
    "    conjunctions = output['word usage']['conjunction']\n",
    "    prepositions = output['word usage']['preposition']\n",
    "    nominalizations = output['word usage']['nominalization']\n",
    "    \n",
    "    feature_scores = []\n",
    "    feature_scores.extend((num_characters, num_words, num_sentences, num_syllables, \n",
    "                           num_unique_words, unique_to_total, avg_characters_word, \n",
    "                           avg_characters_sentence, avg_words_sentence, avg_syllables_word,\n",
    "                           num_polysyllables, num_stopwords, num_positive_sentiment, \n",
    "                           num_negative_sentiment, num_longwords, flesch, kincaid, gunning, smog, \n",
    "                           auto, coleman, linsear, dalechall, lix, rix, be_verbs, aux_verbs, \n",
    "                           conjunctions, prepositions, nominalizations))\n",
    "    \n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the excerpt column in df to numpy array\n",
    "excerpt_arr = pd.DataFrame(train_df['excerpt']).to_numpy()\n",
    "features_scores = []\n",
    "\n",
    "# Create features for all excerpts\n",
    "for e in range(len(excerpt_arr)):\n",
    "    features_scores.append(feature_engineering(excerpt_arr.item(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>unique_to_total</th>\n",
       "      <th>avg_characters_word</th>\n",
       "      <th>avg_characters_sentence</th>\n",
       "      <th>avg_words_sentence</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>...</th>\n",
       "      <th>coleman</th>\n",
       "      <th>linsear</th>\n",
       "      <th>dalechall</th>\n",
       "      <th>lix</th>\n",
       "      <th>rix</th>\n",
       "      <th>be_verbs</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>992</td>\n",
       "      <td>174</td>\n",
       "      <td>6</td>\n",
       "      <td>230</td>\n",
       "      <td>112</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>4.706897</td>\n",
       "      <td>164.500000</td>\n",
       "      <td>29.833333</td>\n",
       "      <td>1.127451</td>\n",
       "      <td>...</td>\n",
       "      <td>8.06</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>6.5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>937</td>\n",
       "      <td>164</td>\n",
       "      <td>6</td>\n",
       "      <td>228</td>\n",
       "      <td>123</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.719512</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>28.166667</td>\n",
       "      <td>1.022422</td>\n",
       "      <td>...</td>\n",
       "      <td>6.78</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>908</td>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>215</td>\n",
       "      <td>124</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>4.611111</td>\n",
       "      <td>180.800000</td>\n",
       "      <td>33.200000</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>...</td>\n",
       "      <td>7.20</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>6.29</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>909</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>117</td>\n",
       "      <td>0.717791</td>\n",
       "      <td>4.582822</td>\n",
       "      <td>454.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.010309</td>\n",
       "      <td>...</td>\n",
       "      <td>8.54</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.184049</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>723</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>51</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>...</td>\n",
       "      <td>4.83</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_characters  num_words  num_sentences  num_syllables  num_unique_words  \\\n",
       "0             992        174              6            230               112   \n",
       "1             937        164              6            228               123   \n",
       "2             908        162              5            215               124   \n",
       "3             909        163              2            196               117   \n",
       "4             723        147              1            170                51   \n",
       "\n",
       "   unique_to_total  avg_characters_word  avg_characters_sentence  \\\n",
       "0         0.643678             4.706897               164.500000   \n",
       "1         0.750000             4.719512               155.333333   \n",
       "2         0.765432             4.611111               180.800000   \n",
       "3         0.717791             4.582822               454.000000   \n",
       "4         0.346939             3.925170               723.000000   \n",
       "\n",
       "   avg_words_sentence  avg_syllables_word  ...  coleman    linsear  dalechall  \\\n",
       "0           29.833333            1.127451  ...     8.06   9.000000       6.65   \n",
       "1           28.166667            1.022422  ...     6.78   7.285714       5.92   \n",
       "2           33.200000            1.023810  ...     7.20  14.750000       6.29   \n",
       "3           82.000000            1.010309  ...     8.54  12.500000       6.61   \n",
       "4          147.000000            0.971429  ...     4.83  13.500000       1.57   \n",
       "\n",
       "        lix   rix  be_verbs  aux_verbs  conjunctions  prepositions  \\\n",
       "0  0.224138   6.5        12          1            11            23   \n",
       "1  0.201220   5.5         5          5             7            22   \n",
       "2  0.209877   6.8         7          1            11            18   \n",
       "3  0.184049  15.0         1          0            15            26   \n",
       "4  0.068027  10.0         4          0            10            10   \n",
       "\n",
       "   nominalizations  \n",
       "0                1  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert features into a pandas dataframe\n",
    "features_names = ['num_characters', 'num_words', 'num_sentences', 'num_syllables', \n",
    "                  'num_unique_words', 'unique_to_total', 'avg_characters_word', \n",
    "                  'avg_characters_sentence', 'avg_words_sentence', 'avg_syllables_word', \n",
    "                  'num_polysyllables', 'num_stopwords', 'num_positive_sentiment', \n",
    "                  'num_negative_sentiment', 'num_longwords', 'flesch', 'kincaid', 'gunning', 'smog', \n",
    "                  'auto', 'coleman', 'linsear', 'dalechall', 'lix', 'rix', 'be_verbs', 'aux_verbs', \n",
    "                  'conjunctions', 'prepositions', 'nominalizations']\n",
    "features_df = pd.DataFrame(features_scores, columns = features_names)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_words_tokens(excerpt_arr):\n",
    "    \"\"\"\n",
    "    Tokenize the given excerpt_arr (corpus array into tokens) and return the list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add regex to Spacy infix to preserve intra-word concatenators\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    infixes = nlp.Defaults.prefixes + [r'[./]',r\"[-]~\",r\"(.'.)\"]\n",
    "    infixes_re = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer = Tokenizer(nlp.vocab, infix_finditer = infixes_re.finditer)\n",
    "    \n",
    "    # Tokenize the whole excerpt array \n",
    "    words_list = []\n",
    "    for ex in excerpt_arr:\n",
    "        doc = nlp(ex)\n",
    "        tokens = [token for token in doc if not (token.is_punct or token.is_space)]\n",
    "        words_list.append(tokens)\n",
    "    \n",
    "    return words_list\n",
    "\n",
    "\n",
    "def spacy_features(excerpt_arr):\n",
    "    \"\"\"\n",
    "    Create features (related to word contents) using spacy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the given excerpt array before creating features\n",
    "    words_list = spacy_words_tokens(excerpt_arr)\n",
    "    \n",
    "    # Word contents (adjectives, adverbs, nouns, verbs, pronouns)\n",
    "    adjectives = [sum([w.pos_ == 'ADJ' for w in wl]) for wl in words_list]\n",
    "    adverbs = [sum([w.pos_ == 'ADV' for w in wl]) for wl in words_list]\n",
    "    nouns = [sum([w.pos_ == 'NOUN' for w in wl]) for wl in words_list]\n",
    "    verbs = [sum([w.pos_ == 'VERB' for w in wl]) for wl in words_list]\n",
    "    pronouns = [sum([w.pos_ == 'PRON' for w in wl]) for wl in words_list]\n",
    "    total_words = [len(wl) for wl in words_list]\n",
    "    contents = [sum([w.pos_ in ['ADJ','ADV','NOUN','VERB'] for w in wl]) for wl in words_list]\n",
    "    # Measure the proportion of words contents\n",
    "    content_diversity = np.divide(contents, total_words)\n",
    "    \n",
    "    spacy_features_scores = []\n",
    "    spacy_features_scores.extend((adjectives, adverbs, nouns, verbs, pronouns, content_diversity))\n",
    "    \n",
    "    return spacy_features_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>unique_to_total</th>\n",
       "      <th>avg_characters_word</th>\n",
       "      <th>avg_characters_sentence</th>\n",
       "      <th>avg_words_sentence</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>...</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>content_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>992</td>\n",
       "      <td>174</td>\n",
       "      <td>6</td>\n",
       "      <td>230</td>\n",
       "      <td>112</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>4.706897</td>\n",
       "      <td>164.500000</td>\n",
       "      <td>29.833333</td>\n",
       "      <td>1.127451</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.458101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>937</td>\n",
       "      <td>164</td>\n",
       "      <td>6</td>\n",
       "      <td>228</td>\n",
       "      <td>123</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.719512</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>28.166667</td>\n",
       "      <td>1.022422</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.514451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>908</td>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>215</td>\n",
       "      <td>124</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>4.611111</td>\n",
       "      <td>180.800000</td>\n",
       "      <td>33.200000</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>909</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>117</td>\n",
       "      <td>0.717791</td>\n",
       "      <td>4.582822</td>\n",
       "      <td>454.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.010309</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.518293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>723</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>51</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.353741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_characters  num_words  num_sentences  num_syllables  num_unique_words  \\\n",
       "0             992        174              6            230               112   \n",
       "1             937        164              6            228               123   \n",
       "2             908        162              5            215               124   \n",
       "3             909        163              2            196               117   \n",
       "4             723        147              1            170                51   \n",
       "\n",
       "   unique_to_total  avg_characters_word  avg_characters_sentence  \\\n",
       "0         0.643678             4.706897               164.500000   \n",
       "1         0.750000             4.719512               155.333333   \n",
       "2         0.765432             4.611111               180.800000   \n",
       "3         0.717791             4.582822               454.000000   \n",
       "4         0.346939             3.925170               723.000000   \n",
       "\n",
       "   avg_words_sentence  avg_syllables_word  ...  aux_verbs  conjunctions  \\\n",
       "0           29.833333            1.127451  ...          1            11   \n",
       "1           28.166667            1.022422  ...          5             7   \n",
       "2           33.200000            1.023810  ...          1            11   \n",
       "3           82.000000            1.010309  ...          0            15   \n",
       "4          147.000000            0.971429  ...          0            10   \n",
       "\n",
       "   prepositions  nominalizations  adjectives  adverbs  nouns  verbs  pronouns  \\\n",
       "0            23                1        10.0      6.0   44.0   22.0       4.0   \n",
       "1            22                0        13.0     18.0   25.0   33.0      26.0   \n",
       "2            18                0         8.0     14.0   19.0   27.0      23.0   \n",
       "3            26                0        22.0      5.0   38.0   20.0      12.0   \n",
       "4            10                0        23.0      4.0   16.0    9.0       8.0   \n",
       "\n",
       "   content_diversity  \n",
       "0           0.458101  \n",
       "1           0.514451  \n",
       "2           0.400000  \n",
       "3           0.518293  \n",
       "4           0.353741  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy features \n",
    "spacy_features_scores = spacy_features(train_df['excerpt'])\n",
    "\n",
    "# Convert spacy features into a pandas dataframe\n",
    "spacy_features_names = ['adjectives', 'adverbs', 'nouns', 'verbs', 'pronouns', 'content_diversity']\n",
    "spacy_features_df = pd.DataFrame(np.array(spacy_features_scores).T, columns = spacy_features_names)\n",
    "\n",
    "# Concatenate both basic features and spacy features \n",
    "features_df = pd.concat([features_df, spacy_features_df], axis = 1, join = 'outer')\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>...</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>content_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>992</td>\n",
       "      <td>174</td>\n",
       "      <td>6</td>\n",
       "      <td>230</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.458101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>937</td>\n",
       "      <td>164</td>\n",
       "      <td>6</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.514451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>908</td>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>909</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.518293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>723</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.353741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  \\\n",
       "0  When the young people returned to the ballroom... -0.340259   \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "3  And outside before the palace a great garden w... -1.054013   \n",
       "4  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "   standard_error  num_characters  num_words  num_sentences  num_syllables  \\\n",
       "0        0.464009             992        174              6            230   \n",
       "1        0.480805             937        164              6            228   \n",
       "2        0.476676             908        162              5            215   \n",
       "3        0.450007             909        163              2            196   \n",
       "4        0.510845             723        147              1            170   \n",
       "\n",
       "   ...  aux_verbs  conjunctions  prepositions  nominalizations  adjectives  \\\n",
       "0  ...          1            11            23                1        10.0   \n",
       "1  ...          5             7            22                0        13.0   \n",
       "2  ...          1            11            18                0         8.0   \n",
       "3  ...          0            15            26                0        22.0   \n",
       "4  ...          0            10            10                0        23.0   \n",
       "\n",
       "   adverbs  nouns  verbs  pronouns  content_diversity  \n",
       "0      6.0   44.0   22.0       4.0           0.458101  \n",
       "1     18.0   25.0   33.0      26.0           0.514451  \n",
       "2     14.0   19.0   27.0      23.0           0.400000  \n",
       "3      5.0   38.0   20.0      12.0           0.518293  \n",
       "4      4.0   16.0    9.0       8.0           0.353741  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate both original and features dataframes (Use outer for union concatenation)\n",
    "train_df = pd.concat([train_df, features_df], axis = 1, join = 'outer')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Debugging</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    url_legal       license  \\\n",
       "0  c0f722661                                          NaN           NaN   \n",
       "1  f0953f0a5                                          NaN           NaN   \n",
       "2  0df072751                                          NaN           NaN   \n",
       "3  04caf4e0c  https://en.wikipedia.org/wiki/Cell_division  CC BY-SA 3.0   \n",
       "4  0e63f8bea      https://en.wikipedia.org/wiki/Debugging  CC BY-SA 3.0   \n",
       "\n",
       "                                             excerpt  \n",
       "0  My hope lay in Jack's promise that he would ke...  \n",
       "1  Dotty continued to go to Mrs. Gray's every nig...  \n",
       "2  It was a bright and cheerful scene that greete...  \n",
       "3  Cell division is the process by which a parent...  \n",
       "4  Debugging is the process of finding and resolv...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file and print the first 5 rows\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the excerpt column in df to numpy array\n",
    "excerpt_arr = pd.DataFrame(test_df['excerpt']).to_numpy()\n",
    "features_scores2 = []\n",
    "\n",
    "# Create features for all excerpts\n",
    "for e in range(len(excerpt_arr)):\n",
    "    features_scores2.append(feature_engineering(excerpt_arr.item(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>unique_to_total</th>\n",
       "      <th>avg_characters_word</th>\n",
       "      <th>avg_characters_sentence</th>\n",
       "      <th>avg_words_sentence</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>...</th>\n",
       "      <th>coleman</th>\n",
       "      <th>linsear</th>\n",
       "      <th>dalechall</th>\n",
       "      <th>lix</th>\n",
       "      <th>rix</th>\n",
       "      <th>be_verbs</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>772</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199</td>\n",
       "      <td>102</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>4.258503</td>\n",
       "      <td>256.666667</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>1.184524</td>\n",
       "      <td>...</td>\n",
       "      <td>6.62</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>967</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "      <td>218</td>\n",
       "      <td>121</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>4.407821</td>\n",
       "      <td>321.666667</td>\n",
       "      <td>60.333333</td>\n",
       "      <td>1.023474</td>\n",
       "      <td>...</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.428571</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>948</td>\n",
       "      <td>168</td>\n",
       "      <td>7</td>\n",
       "      <td>226</td>\n",
       "      <td>122</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>4.648810</td>\n",
       "      <td>134.571429</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.041475</td>\n",
       "      <td>...</td>\n",
       "      <td>7.61</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.78</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1144</td>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>312</td>\n",
       "      <td>106</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>5.396648</td>\n",
       "      <td>285.250000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>1.536946</td>\n",
       "      <td>...</td>\n",
       "      <td>13.70</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>9.55</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1094</td>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>124</td>\n",
       "      <td>0.742515</td>\n",
       "      <td>5.556886</td>\n",
       "      <td>546.500000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.432836</td>\n",
       "      <td>...</td>\n",
       "      <td>13.93</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.401198</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_characters  num_words  num_sentences  num_syllables  num_unique_words  \\\n",
       "0             772        147              3            199               102   \n",
       "1             967        179              3            218               121   \n",
       "2             948        168              7            226               122   \n",
       "3            1144        179              4            312               106   \n",
       "4            1094        167              2            288               124   \n",
       "\n",
       "   unique_to_total  avg_characters_word  avg_characters_sentence  \\\n",
       "0         0.693878             4.258503               256.666667   \n",
       "1         0.675978             4.407821               321.666667   \n",
       "2         0.726190             4.648810               134.571429   \n",
       "3         0.592179             5.396648               285.250000   \n",
       "4         0.742515             5.556886               546.500000   \n",
       "\n",
       "   avg_words_sentence  avg_syllables_word  ...  coleman    linsear  dalechall  \\\n",
       "0           49.666667            1.184524  ...     6.62  11.000000       6.24   \n",
       "1           60.333333            1.023474  ...     6.55   6.428571       5.41   \n",
       "2           24.857143            1.041475  ...     7.61  14.000000       6.78   \n",
       "3           45.500000            1.536946  ...    13.70  17.250000       9.55   \n",
       "4           84.000000            1.432836  ...    13.93  15.200000      10.01   \n",
       "\n",
       "        lix        rix  be_verbs  aux_verbs  conjunctions  prepositions  \\\n",
       "0  0.204082  10.000000         4          3             4            27   \n",
       "1  0.134078   8.000000         5          6             9            21   \n",
       "2  0.202381   4.857143         8          0            11            15   \n",
       "3  0.363128  16.250000        11          0             4            28   \n",
       "4  0.401198  33.500000         8          1             7            22   \n",
       "\n",
       "   nominalizations  \n",
       "0                1  \n",
       "1                0  \n",
       "2                0  \n",
       "3                2  \n",
       "4                5  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df2 = pd.DataFrame(features_scores2, columns = features_names)\n",
    "features_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>unique_to_total</th>\n",
       "      <th>avg_characters_word</th>\n",
       "      <th>avg_characters_sentence</th>\n",
       "      <th>avg_words_sentence</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>...</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>content_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>772</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199</td>\n",
       "      <td>102</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>4.258503</td>\n",
       "      <td>256.666667</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>1.184524</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.446667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>967</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "      <td>218</td>\n",
       "      <td>121</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>4.407821</td>\n",
       "      <td>321.666667</td>\n",
       "      <td>60.333333</td>\n",
       "      <td>1.023474</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>948</td>\n",
       "      <td>168</td>\n",
       "      <td>7</td>\n",
       "      <td>226</td>\n",
       "      <td>122</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>4.648810</td>\n",
       "      <td>134.571429</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.041475</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.401130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1144</td>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>312</td>\n",
       "      <td>106</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>5.396648</td>\n",
       "      <td>285.250000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>1.536946</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1094</td>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>124</td>\n",
       "      <td>0.742515</td>\n",
       "      <td>5.556886</td>\n",
       "      <td>546.500000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.432836</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_characters  num_words  num_sentences  num_syllables  num_unique_words  \\\n",
       "0             772        147              3            199               102   \n",
       "1             967        179              3            218               121   \n",
       "2             948        168              7            226               122   \n",
       "3            1144        179              4            312               106   \n",
       "4            1094        167              2            288               124   \n",
       "\n",
       "   unique_to_total  avg_characters_word  avg_characters_sentence  \\\n",
       "0         0.693878             4.258503               256.666667   \n",
       "1         0.675978             4.407821               321.666667   \n",
       "2         0.726190             4.648810               134.571429   \n",
       "3         0.592179             5.396648               285.250000   \n",
       "4         0.742515             5.556886               546.500000   \n",
       "\n",
       "   avg_words_sentence  avg_syllables_word  ...  aux_verbs  conjunctions  \\\n",
       "0           49.666667            1.184524  ...          3             4   \n",
       "1           60.333333            1.023474  ...          6             9   \n",
       "2           24.857143            1.041475  ...          0            11   \n",
       "3           45.500000            1.536946  ...          0             4   \n",
       "4           84.000000            1.432836  ...          1             7   \n",
       "\n",
       "   prepositions  nominalizations  adjectives  adverbs  nouns  verbs  pronouns  \\\n",
       "0            27                1        13.0      6.0   28.0   20.0      15.0   \n",
       "1            21                0         6.0     14.0   29.0   31.0      20.0   \n",
       "2            15                0        11.0     14.0   28.0   18.0      27.0   \n",
       "3            28                2        19.0      9.0   63.0   17.0       3.0   \n",
       "4            22                5         9.0      8.0   44.0   27.0       5.0   \n",
       "\n",
       "   content_diversity  \n",
       "0           0.446667  \n",
       "1           0.434783  \n",
       "2           0.401130  \n",
       "3           0.600000  \n",
       "4           0.523810  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spacy features \n",
    "spacy_features_scores2 = spacy_features(test_df['excerpt'])\n",
    "\n",
    "# Convert spacy features into a pandas dataframe\n",
    "spacy_features_df2 = pd.DataFrame(np.array(spacy_features_scores2).T, columns = spacy_features_names)\n",
    "\n",
    "# Concatenate both basic features and spacy features \n",
    "features_df2 = pd.concat([features_df2, spacy_features_df2], axis = 1, join = 'outer')\n",
    "features_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>unique_to_total</th>\n",
       "      <th>...</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>conjunctions</th>\n",
       "      <th>prepositions</th>\n",
       "      <th>nominalizations</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>content_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "      <td>772</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199</td>\n",
       "      <td>102</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.446667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "      <td>967</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "      <td>218</td>\n",
       "      <td>121</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "      <td>948</td>\n",
       "      <td>168</td>\n",
       "      <td>7</td>\n",
       "      <td>226</td>\n",
       "      <td>122</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.401130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "      <td>1144</td>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>312</td>\n",
       "      <td>106</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Debugging</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "      <td>1094</td>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>124</td>\n",
       "      <td>0.742515</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    url_legal       license  \\\n",
       "0  c0f722661                                          NaN           NaN   \n",
       "1  f0953f0a5                                          NaN           NaN   \n",
       "2  0df072751                                          NaN           NaN   \n",
       "3  04caf4e0c  https://en.wikipedia.org/wiki/Cell_division  CC BY-SA 3.0   \n",
       "4  0e63f8bea      https://en.wikipedia.org/wiki/Debugging  CC BY-SA 3.0   \n",
       "\n",
       "                                             excerpt  num_characters  \\\n",
       "0  My hope lay in Jack's promise that he would ke...             772   \n",
       "1  Dotty continued to go to Mrs. Gray's every nig...             967   \n",
       "2  It was a bright and cheerful scene that greete...             948   \n",
       "3  Cell division is the process by which a parent...            1144   \n",
       "4  Debugging is the process of finding and resolv...            1094   \n",
       "\n",
       "   num_words  num_sentences  num_syllables  num_unique_words  unique_to_total  \\\n",
       "0        147              3            199               102         0.693878   \n",
       "1        179              3            218               121         0.675978   \n",
       "2        168              7            226               122         0.726190   \n",
       "3        179              4            312               106         0.592179   \n",
       "4        167              2            288               124         0.742515   \n",
       "\n",
       "   ...  aux_verbs  conjunctions  prepositions  nominalizations  adjectives  \\\n",
       "0  ...          3             4            27                1        13.0   \n",
       "1  ...          6             9            21                0         6.0   \n",
       "2  ...          0            11            15                0        11.0   \n",
       "3  ...          0             4            28                2        19.0   \n",
       "4  ...          1             7            22                5         9.0   \n",
       "\n",
       "   adverbs  nouns  verbs  pronouns  content_diversity  \n",
       "0      6.0   28.0   20.0      15.0           0.446667  \n",
       "1     14.0   29.0   31.0      20.0           0.434783  \n",
       "2     14.0   28.0   18.0      27.0           0.401130  \n",
       "3      9.0   63.0   17.0       3.0           0.600000  \n",
       "4      8.0   44.0   27.0       5.0           0.523810  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate both original and features dataframes (Use outer for union concatenation)\n",
    "test_df = pd.concat([test_df, features_df2], axis = 1, join = 'outer')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
